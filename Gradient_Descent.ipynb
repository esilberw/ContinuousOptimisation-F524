{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MnoF3mpotE9A",
        "outputId": "c95715f1-d26d-4b0d-de49-a38e3014ca38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0  tmax  tmin  rain  tmax_tomorrow\n",
              "0  1970-01-01  60.0  35.0   0.0           52.0\n",
              "1  1970-01-02  52.0  39.0   0.0           52.0\n",
              "2  1970-01-03  52.0  35.0   0.0           53.0\n",
              "3  1970-01-04  53.0  36.0   0.0           52.0\n",
              "4  1970-01-05  52.0  35.0   0.0           50.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f7327b20-4d53-489c-bbb5-83991160806d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>tmax</th>\n",
              "      <th>tmin</th>\n",
              "      <th>rain</th>\n",
              "      <th>tmax_tomorrow</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1970-01-01</td>\n",
              "      <td>60.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1970-01-02</td>\n",
              "      <td>52.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1970-01-03</td>\n",
              "      <td>52.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1970-01-04</td>\n",
              "      <td>53.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>52.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1970-01-05</td>\n",
              "      <td>52.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7327b20-4d53-489c-bbb5-83991160806d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f7327b20-4d53-489c-bbb5-83991160806d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f7327b20-4d53-489c-bbb5-83991160806d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f42eaffe-443b-4dcc-9217-cfd385e5d7b1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f42eaffe-443b-4dcc-9217-cfd385e5d7b1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f42eaffe-443b-4dcc-9217-cfd385e5d7b1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 13509,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 13509,\n        \"samples\": [\n          \"2004-11-23\",\n          \"1984-09-12\",\n          \"2020-07-07\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tmax\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.321769458469971,\n        \"min\": 38.0,\n        \"max\": 122.0,\n        \"num_unique_values\": 66,\n        \"samples\": [\n          43.0,\n          122.0,\n          60.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tmin\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.78579770467797,\n        \"min\": 1.0,\n        \"max\": 69.0,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          37.0,\n          68.0,\n          40.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rain\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18153310034630646,\n        \"min\": 0.0,\n        \"max\": 3.58,\n        \"num_unique_values\": 161,\n        \"samples\": [\n          1.29,\n          0.61,\n          1.52\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tmax_tomorrow\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.321625140373289,\n        \"min\": 38.0,\n        \"max\": 122.0,\n        \"num_unique_values\": 66,\n        \"samples\": [\n          43.0,\n          122.0,\n          52.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"./clean_weather.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert categorical to numeric\n",
        "#df['Extracurricular Activities'] = df['Extracurricular Activities'].map({\n",
        "                                                                        #'Yes': 1, 'No': 0})\n",
        "\n",
        "# Extract features and target\n",
        "X = df.drop(columns=['tmax_tomorrow', 'Unnamed: 0']).astype(float)\n",
        "y = df['tmax_tomorrow'].astype(float)\n",
        "# Replace NaN values in X with column means\n",
        "col_means = np.nanmean(X, axis=0)  # Compute mean without NaNs\n",
        "X = np.where(np.isnan(X), col_means, X)  # Replace NaNs with mean\n",
        "\n",
        "# Replace NaNs in y with mean value\n",
        "y = np.where(np.isnan(y), np.nanmean(y), y)\n",
        "\n",
        "# # Compute mean and standard deviation of each feature\n",
        "# X_mean = np.mean(X, axis=0)\n",
        "# X_std = np.std(X, axis=0)\n",
        "\n",
        "# # Standardize: (X - mean) / std\n",
        "# X_standardized = (X - X_mean) / X_std\n",
        "\n",
        "# # Ensure no division by zero (replace std=0 with 1 to avoid NaNs)\n",
        "# X_standardized[:, X_std == 0] = 0\n",
        "\n",
        "\n",
        "# Get the shapes\n",
        "#X_np.shape, y_np.shape\n",
        "\n",
        "# Add bias term (intercept) - a column of ones\n",
        "X = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "# Split into train/test sets\n",
        "def train_test_split(X, y, test_ratio=0.2, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    indices = np.random.permutation(X.shape[0])\n",
        "    test_size = int(len(X) * test_ratio)\n",
        "    test_idx, train_idx = indices[:test_size], indices[test_size:]\n",
        "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u6SvHs7tXfc",
        "outputId": "8b16d1a9-90f9-4b3c-e631-ad59f657f24b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10808, 4), (2701, 4), (10808,), (2701,))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(beta,X,y):\n",
        "        \"\"\"Compute MSE loss.\"\"\"\n",
        "        return np.mean((X @ beta - y) ** 2)"
      ],
      "metadata": {
        "id": "ePc5gGfytW2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backtracking_line_search(X, y, beta, grad, eta_init=0.5, alpha=0.1, c=1e-4):\n",
        "    \"\"\"\n",
        "    Implements Backtracking Line Search to determine step size.\n",
        "\n",
        "    Parameters:\n",
        "    X : np.array -> Feature matrix\n",
        "    y : np.array -> Target values\n",
        "    beta : np.array -> Current coefficients\n",
        "    grad : np.array -> Gradient at beta\n",
        "    eta_init : float -> Initial step size (typically 1)\n",
        "    alpha : float -> Contraction factor (typically 0.5)\n",
        "    c : float -> Armijo condition parameter\n",
        "\n",
        "    Returns:\n",
        "    eta : float -> Optimized step size\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    eta = eta_init\n",
        "    loss_old = loss(beta, X,y)\n",
        "    direction = -grad  # d_k = -âˆ‡f(x_k)\n",
        "\n",
        "    while True:\n",
        "        beta_new = beta + eta * direction  # Test new beta\n",
        "        loss_new = loss(beta_new,X,y)\n",
        "        print(f\"Loss Old: {loss_old}, Loss New: {loss_new}\")\n",
        "\n",
        "        # Check Armijo condition\n",
        "        if loss_new <= loss_old + c * eta * np.dot(grad, direction):\n",
        "            break  # Stop if sufficient decrease\n",
        "\n",
        "        eta *= alpha  # Reduce step size if condition is not met\n",
        "\n",
        "    return eta\n"
      ],
      "metadata": {
        "id": "Nl8yLFJvvdFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_backtracking(X, y, max_iter=100000, epsilon = 1e-5):\n",
        "    \"\"\"\n",
        "    Gradient Descent with Backtracking Line Search.\n",
        "    \"\"\"\n",
        "\n",
        "    n, d = X.shape\n",
        "    beta = np.zeros(d)  # Initialize beta\n",
        "\n",
        "    def compute_gradient(beta, X, y, h=1e-5):\n",
        "        #\"\"\"Compute gradient of MSE loss.\"\"\"\n",
        "        return (2 / n) * X.T @ (X @ beta - y)\n",
        "\n",
        "        # \"\"\"Compute gradient using finite differences approximation.\"\"\"\n",
        "        # d = len(beta)\n",
        "        # grad = np.zeros(d)\n",
        "\n",
        "        # for i in range(d):\n",
        "        #     e_i = np.zeros(d)\n",
        "        #     e_i[i] = h  # Unit vector along i-th direction\n",
        "\n",
        "        #     # Compute finite difference approximation of the gradient\n",
        "        #     grad[i] = (loss(beta + e_i, X, y) - loss(beta, X, y)) / h\n",
        "\n",
        "        # return grad\n",
        "    i=1\n",
        "    while(True):\n",
        "        print(f\"Iteration {i+1}:\")\n",
        "        grad = compute_gradient(beta, X, y)  # Compute gradient\n",
        "        eta = backtracking_line_search(X, y, beta, grad)  # Adapt step size\n",
        "        beta_new = beta - eta * grad  # Update beta\n",
        "\n",
        "        norm = np.linalg.norm(grad)\n",
        "        print(f\"gradient norm: {norm}\")\n",
        "        # ðŸš¨ Stopping Criterion: If gradient norm is very small, stop\n",
        "        if norm < epsilon:\n",
        "            print(f\"Converged in {i} iterations!\")\n",
        "            break\n",
        "\n",
        "\n",
        "        beta = beta_new\n",
        "        i+=1\n",
        "\n",
        "    return beta\n"
      ],
      "metadata": {
        "id": "_hBFANu0vlil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta_opt = gradient_descent_backtracking(X_train, y_train)\n",
        "print(\"Optimized Coefficients:\", beta_opt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b7-CRzNFvpUR",
        "outputId": "a8e80aed-dac4-477a-aeb8-1888fef47ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189790:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189791:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189792:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189793:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189794:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189795:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189796:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189797:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189798:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189799:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189800:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189801:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189802:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189803:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189804:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189805:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189806:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189807:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189808:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189809:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189810:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189811:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189812:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189813:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189814:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189815:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189816:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189817:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189818:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189819:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189820:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189821:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189822:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189823:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189824:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189825:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189826:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189827:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189828:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189829:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189830:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189831:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189832:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189833:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189834:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189835:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189836:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189837:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189838:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189839:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189840:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189841:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189842:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189843:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189844:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189845:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189846:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189847:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189848:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189849:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189850:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189851:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189852:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189853:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189854:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189855:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189856:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189857:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189858:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189859:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189860:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189861:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189862:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189863:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189864:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189865:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189866:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189867:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189868:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189869:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189870:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189871:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189872:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189873:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189874:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189875:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189876:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189877:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189878:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189879:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189880:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189881:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189882:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189883:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189884:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189885:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189886:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189887:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189888:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189889:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189890:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189891:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189892:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189893:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189894:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189895:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189896:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189897:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189898:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189899:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189900:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189901:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189902:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189903:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189904:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189905:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189906:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189907:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189908:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189909:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189910:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189911:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189912:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189913:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189914:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189915:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189916:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189917:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189918:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189919:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189920:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189921:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189922:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189923:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189924:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189925:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189926:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189927:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189928:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189929:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189930:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189931:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189932:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189933:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189934:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189935:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189936:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189937:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189938:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189939:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189940:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189941:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189942:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189943:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189944:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189945:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189946:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189947:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189948:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189949:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189950:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189951:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189952:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189953:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189954:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189955:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189956:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189957:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189958:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189959:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189960:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189961:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189962:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189963:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189964:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189965:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189966:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189967:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189968:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189969:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189970:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189971:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189972:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189973:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189974:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189975:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189976:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189977:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189978:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189979:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189980:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189981:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189982:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189983:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189984:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189985:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189986:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189987:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189988:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189989:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189990:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189991:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189992:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189993:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189994:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189995:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189996:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189997:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189998:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10189999:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190000:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190001:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190002:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190003:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190004:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190005:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190006:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190007:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190008:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190009:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190010:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190011:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190012:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190013:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190014:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190015:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190016:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190017:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190018:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190019:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190020:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190021:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190022:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190023:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190024:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190025:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190026:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190027:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190028:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190029:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190030:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190031:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190032:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190033:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190034:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190035:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190036:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190037:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190038:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190039:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190040:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190041:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190042:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190043:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190044:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190045:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190046:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190047:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190048:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190049:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190050:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190051:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190052:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190053:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190054:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190055:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190056:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190057:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190058:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190059:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190060:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190061:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190062:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190063:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190064:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190065:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190066:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190067:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190068:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190069:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190070:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190071:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190072:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190073:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190074:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190075:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190076:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190077:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190078:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190079:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190080:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190081:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190082:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190083:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190084:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190085:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190086:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190087:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190088:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190089:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190090:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190091:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190092:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190093:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190094:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190095:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190096:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190097:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190098:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190099:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190100:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190101:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190102:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190103:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190104:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190105:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190106:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190107:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190108:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190109:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190110:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190111:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190112:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190113:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190114:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190115:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190116:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190117:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190118:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190119:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190120:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190121:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190122:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190123:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190124:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190125:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190126:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190127:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190128:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190129:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190130:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190131:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190132:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190133:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190134:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190135:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190136:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190137:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190138:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190139:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190140:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190141:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190142:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190143:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190144:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190145:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057526\n",
            "gradient norm: 8.401209047985338e-06\n",
            "Iteration 10190146:\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408981598217\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980072306\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057615\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n",
            "Loss Old: 21.533408980057526, Loss New: 21.53340898005753\n",
            "Loss Old: 21.533408980057526, Loss New: 21.533408980057533\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-694baca82b6a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbeta_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent_backtracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimized Coefficients:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-a3385402c1c9>\u001b[0m in \u001b[0;36mgradient_descent_backtracking\u001b[0;34m(X, y, max_iter, epsilon)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Iteration {i+1}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbacktracking_line_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adapt step size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mbeta_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m  \u001b[0;31m# Update beta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-9b9e19e75d63>\u001b[0m in \u001b[0;36mbacktracking_line_search\u001b[0;34m(X, y, beta, grad, eta_init, alpha, c)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mbeta_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdirection\u001b[0m  \u001b[0;31m# Test new beta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta_new\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss Old: {loss_old}, Loss New: {loss_new}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-545a81d499ac>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(beta, X, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m         \u001b[0;34m\"\"\"Compute MSE loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_mean_dispatcher\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3471\u001b[0;31m def _mean_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None, *,\n\u001b[0m\u001b[1;32m   3472\u001b[0m                      where=None):\n\u001b[1;32m   3473\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def prediction_accuracy(y_true, y_pred):\n",
        "    \"\"\"Computes prediction accuracy based on MSE and variance of y.\"\"\"\n",
        "    mse = np.mean((y_true - y_pred) ** 2)\n",
        "    var_y = np.var(y_true)  # Variance of y\n",
        "\n",
        "    # Avoid division by zero\n",
        "    if var_y == 0:\n",
        "        print(\"Variance of y is zero! Accuracy is undefined.\")\n",
        "        return None\n",
        "\n",
        "    accuracy = (1 - (mse / var_y)) * 100  # Convert to percentage\n",
        "\n",
        "    print(f\"MSE: {mse:.4f}\")\n",
        "    print(f\"Variance of y: {var_y:.4f}\")\n",
        "    print(f\"Prediction Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# Example usage\n",
        "#beta_opt = [ 8.69529915 , 0.73503273,  0.17648365, -2.00327188]\n",
        "y_pred = X_test @ beta_opt  # Predicted values using trained beta\n",
        "prediction_accuracy(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "byrSEb0rvse2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5d50c27-572f-4009-8e4a-0534fa210487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 24.1406\n",
            "Variance of y: 70.5146\n",
            "Prediction Accuracy: 65.77%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(65.76504419911046)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def backtracking_line_search(X, y, beta, grad, eta_init=1, alpha=0.5, c=1e-4):\n",
        "    \"\"\"Backtracking line search with safe loss computation.\"\"\"\n",
        "\n",
        "    def loss(beta):\n",
        "        try:\n",
        "            return np.mean((X @ beta - y) ** 2)\n",
        "        except:\n",
        "            return np.inf\n",
        "\n",
        "    eta = eta_init\n",
        "    loss_old = loss(beta)\n",
        "    direction = -grad / np.linalg.norm(grad)  # Normalize gradient\n",
        "\n",
        "    while True:\n",
        "        beta_new = beta + eta * direction\n",
        "        loss_new = loss(beta_new)\n",
        "\n",
        "        if np.isnan(loss_new):\n",
        "            eta *= alpha\n",
        "            continue\n",
        "        print(f\"Loss Old: {loss_old}, Loss New: {loss_new}\")\n",
        "        if loss_new <= loss_old + c * eta * np.dot(grad, direction):\n",
        "            break\n",
        "\n",
        "        eta *= alpha\n",
        "        if eta < 1e-10:\n",
        "            break\n",
        "\n",
        "    return eta\n",
        "\n",
        "def gradient_descent_visualize(X, y, max_iter=1000, tol=1e-6):\n",
        "    \"\"\"Gradient Descent with Backtracking + Visualization.\"\"\"\n",
        "\n",
        "    n, d = X.shape\n",
        "    beta = np.zeros(d)\n",
        "    loss_history = []\n",
        "    step_sizes = []\n",
        "    grad_norms = []\n",
        "\n",
        "    def compute_gradient(beta, X, y, h=1e-5):\n",
        "        #\"\"\"Compute gradient of MSE loss.\"\"\"\n",
        "        #return (2 / n) * X.T @ (X @ beta - y)\n",
        "\n",
        "        \"\"\"Compute gradient using finite differences approximation.\"\"\"\n",
        "        d = len(beta)\n",
        "        grad = np.zeros(d)\n",
        "\n",
        "    def loss(beta):\n",
        "        return np.mean((X @ beta - y) ** 2)\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        grad = compute_gradient(beta, X,y)\n",
        "        grad_norms.append(np.linalg.norm(grad, ord=2))\n",
        "\n",
        "        eta = backtracking_line_search(X, y, beta, grad)\n",
        "        step_sizes.append(eta)\n",
        "\n",
        "        beta_new = beta - eta * grad\n",
        "        loss_history.append(loss(beta_new))\n",
        "\n",
        "        if np.linalg.norm(beta_new - beta, ord=2) < tol:\n",
        "            print(f\"Converged in {i+1} iterations.\")\n",
        "            break\n",
        "\n",
        "        beta = beta_new\n",
        "\n",
        "    # ðŸ“Š Plot Results\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    ax[0].plot(loss_history, label=\"Loss\")\n",
        "    ax[0].set_title(\"Loss Over Iterations\")\n",
        "    ax[0].set_xlabel(\"Iteration\")\n",
        "    ax[0].set_ylabel(\"Loss\")\n",
        "    ax[0].legend()\n",
        "\n",
        "    ax[1].plot(step_sizes, label=\"Step Size (Î·)\", color=\"orange\")\n",
        "    ax[1].set_title(\"Step Size Over Iterations\")\n",
        "    ax[1].set_xlabel(\"Iteration\")\n",
        "    ax[1].set_ylabel(\"Step Size\")\n",
        "    ax[1].legend()\n",
        "\n",
        "    ax[2].plot(grad_norms, label=\"Gradient Norm\", color=\"red\")\n",
        "    ax[2].set_title(\"Gradient Norm Over Iterations\")\n",
        "    ax[2].set_xlabel(\"Iteration\")\n",
        "    ax[2].set_ylabel(\"||âˆ‡f(x)||\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return beta\n",
        "\n",
        "# Run gradient descent with visualization\n",
        "beta_opt = gradient_descent_visualize(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "wWB4ZEU0bz_0",
        "outputId": "89fdf51d-536b-4752-f7bf-652d83306a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Improper number of dimensions to norm.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d49b1f3f033e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;31m# Run gradient descent with visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mbeta_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent_visualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-d49b1f3f033e>\u001b[0m in \u001b[0;36mgradient_descent_visualize\u001b[0;34m(X, y, max_iter, tol)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mgrad_norms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbacktracking_line_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/linalg/_linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2821\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2822\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2823\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Improper number of dimensions to norm.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Improper number of dimensions to norm."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, lr=0.01, max_iter=1000, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Performs gradient descent for linear regression.\n",
        "    X: Feature matrix (with bias term)\n",
        "    y: Target values\n",
        "    lr: Learning rate\n",
        "    max_iter: Max iterations\n",
        "    tol: Convergence tolerance\n",
        "    \"\"\"\n",
        "    n, d = X.shape  # Number of samples (n) and features (d)\n",
        "    beta = np.zeros(d)  # Initialize coefficients\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        gradient = (1 / n) * X.T @ (X @ beta - y)  # Compute gradient\n",
        "        beta_new = beta - lr * gradient  # Update beta\n",
        "\n",
        "        # Check convergence\n",
        "        if np.linalg.norm(beta_new - beta, ord=2) < tol:\n",
        "            print(f\"Converged in {i+1} iterations.\")\n",
        "            break\n",
        "\n",
        "        beta = beta_new  # Update beta for next iteration\n",
        "\n",
        "    return beta\n"
      ],
      "metadata": {
        "id": "9OBHSn-lt6no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beta_opt = gradient_descent(X_train, y_train, lr=0.01, max_iter=1000)\n",
        "print(\"Optimized Coefficients:\", beta_opt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbSKlitxt-dt",
        "outputId": "fbec3e77-e8a8-47bb-b291-fb862e7ce87b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Coefficients: [nan nan nan nan nan nan]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = X_test @ beta_opt  # Predictions\n",
        "\n",
        "# Compute Mean Squared Error (MSE)\n",
        "mse = np.mean((y_test - y_pred) ** 2)\n",
        "print(\"Test MSE:\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtSVtDOeuOZg",
        "outputId": "deb43a11-6fd9-4aef-ef56-39aa703617a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Soft-thresholding operator for L1 regularization\n",
        "def soft_thresholding(z, alpha):\n",
        "    \"\"\"\n",
        "    Applies the soft-thresholding operator element-wise.\n",
        "\n",
        "    Parameters:\n",
        "    - z: input array\n",
        "    - alpha: threshold parameter\n",
        "\n",
        "    Returns:\n",
        "    - Thresholded array\n",
        "    \"\"\"\n",
        "    return np.sign(z) * np.maximum(np.abs(z) - alpha, 0)\n",
        "\n",
        "# ISTA algorithm for solving the LASSO problem\n",
        "def ista(X, y, lambda_, step_size, max_iter=1000, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Iterative Soft-Thresholding Algorithm (ISTA) to solve:\n",
        "        min_beta (1/2n)||XÎ² - y||Â² + Î»||Î²||â‚\n",
        "\n",
        "    Parameters:\n",
        "    - X: Feature matrix (standardized)\n",
        "    - y: Target vector\n",
        "    - lambda_: Regularization parameter controlling sparsity\n",
        "    - step_size: Learning rate (must be <= 1 / Lipschitz constant of âˆ‡f)\n",
        "    - max_iter: Maximum number of iterations\n",
        "    - tol: Convergence tolerance for stopping criterion\n",
        "\n",
        "    Returns:\n",
        "    - beta: Estimated coefficient vector\n",
        "    \"\"\"\n",
        "    n, p = X.shape\n",
        "    beta = np.zeros(p)  # Initialize coefficients to zeros\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        # Compute gradient of the smooth part: (1/n) * Xáµ—(XÎ² - y)\n",
        "        gradient = (1 / n) * X.T @ (X @ beta - y)\n",
        "\n",
        "        # Gradient descent step followed by soft-thresholding\n",
        "        beta_new = soft_thresholding(beta - step_size * gradient, lambda_ * step_size)\n",
        "\n",
        "        # Check convergence using L2 norm of change in beta\n",
        "        if np.linalg.norm(beta_new - beta, ord=2) < tol:\n",
        "            print(f\"Converged in {i} iterations.\")\n",
        "            break\n",
        "\n",
        "        beta = beta_new  # Update beta for next iteration\n",
        "\n",
        "    return beta\n",
        "\n",
        "# Set hyperparameters\n",
        "#lambda_ = 0.1  # Regularization strength\n",
        "# Compute safe step size: 1 / Lipschitz constant of gradient\n",
        "#step_size = 1.0 / np.linalg.norm(X_np.T @ X_np / len(X_np), 2)\n",
        "\n",
        "# Run ISTA algorithm\n",
        "#beta_est = ista(X_np, y_np, lambda_=lambda_, step_size=step_size)\n",
        "\n",
        "# Display the estimated coefficients\n",
        "#print(beta_est)\n"
      ],
      "metadata": {
        "id": "Jv_-ylSvtihp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Split data into train and test\n",
        "def train_test_split(X, y, test_ratio=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    Randomly splits data into training and testing sets.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "    n = X.shape[0]\n",
        "    indices = np.random.permutation(n)\n",
        "    test_size = int(n * test_ratio)\n",
        "    test_idx = indices[:test_size]\n",
        "    train_idx = indices[test_size:]\n",
        "\n",
        "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, test_ratio=0.2)\n",
        "\n",
        "# Step 2: Recompute step size for training set\n",
        "n_train = X_train.shape[0]\n",
        "step_size = 1.0 / np.linalg.norm(X_train.T @ X_train / n_train, 2)\n",
        "\n",
        "# Step 3: Fit ISTA on training set\n",
        "beta_est = ista(X_train, y_train, lambda_=0.1, step_size=step_size)\n",
        "\n",
        "# Step 4: Predict on test set\n",
        "y_pred = X_test @ beta_est\n",
        "\n",
        "# Step 5: Evaluate performance (Mean Squared Error)\n",
        "mse = np.mean((y_test - y_pred) ** 2)\n",
        "\n",
        "print(\"Estimated Coefficients:\", beta_est)\n",
        "print(\"Test MSE:\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4G0AjVAzoNb",
        "outputId": "7e21e8ac-9156-4272-f508-6adbf5a78908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged in 6 iterations.\n",
            "Estimated Coefficients: [ 7.07397718 17.8005892   0.19128215  1.06695716  1.09442799]\n",
            "Test MSE: 3051.635133463572\n"
          ]
        }
      ]
    }
  ]
}